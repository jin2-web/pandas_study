











# ▶ Data read
import pandas as pd


df_features = pd.read_csv('./data/abnormal_features.csv')
df_features.head(2)


df_target = pd.read_csv('./data/abnormal_target.csv')
df_target.head(2)














# ▶ Data 형태 확인
print( 'df_features', df_features.shape)
# ▶ feature data의 row의 개수와 target data의 row 개수가 다름 
print( 'df_target', df_target.shape )


# ▶ Data type 확인
df_features.info()


# ▶ data 정보확인
df_target.info()


# ▶ Null 값 확인
print( df_features.isnull().sum() ) # isnull() True False .sum() True의 갯수
print( df_target.isnull().sum() )


# ▶ Unique한 ID 개수 확인
print("df_features_unique_id :", len( df_features['id'].unique() ) )
print("df_target_id:", len( df_target['id'].unique() ) )
# ▶ Unique한 ID 개수가 동일한 것으로 보아, Feature data가 id당 다수의 데이터를 가지고 있는 것으로 추정



df_features.head()


# ▶ 하나의 ID(제품)당 0.000004초의 간격으로 센서 S1~S4에 값이 수집되고 있는 상황
# id가 0인 것의 5개만 보기
df_features[ df_features['id'] == 0].head()


# ▶ Target data set과 차원을 맞춰주기 위해 id 별로 Sensor 값의 평균(mean)을 사용
# ▶ Data를 그룹핑 하는 방법에는 다양한 방법이 있으나, 현업에서 의미있는 방법으로 데이터를 그룹핑 하는 것을 추천
df_features_proc = df_features.groupby('id').mean()
df_features_proc





print( "df_features ", df_features_proc.shape )
print( "df_target", df_target.shape ) 



# ▶ Data left join



# 자료형태
print(df_features_proc.head())
print(df_target.head())
df_features_proc


df_merge = pd.merge( df_features_proc, df_target, on='id', how='left' )
df_merge.head()


# ▶ 전처리 완료된 최종 Data set
df_merge = df_merge.drop( ['Time','X', 'Y' ], axis = 1 )
df_merge





# 연습용
# 'M'열을 추출하기
df_merge[ 'MV' ] = df_merge['M'] * df_merge['V']


# ▶ 위에서 정의한 것처럼 M(질량) X V(속도) = 최종 충돌 에너지로 정의하고, 새로운 Col을 생성
df_merge[ 'MV' ] = df_merge['M'] * df_merge['V']
df_merge.head()


# 연습
import matplotlib.pyplot as plt 
import seaborn as sns 
get_ipython().run_line_magic("matplotlib", " inline")
# 바로 챠트를 반영해서 보여주기
plt.style.use(['dark_background'])

sns.displot(df_merge['MV'])
plt.gcf().set_size_inches(20,5)
print("총돌에너지 평균 :", df_merge['MV'].mean())


# ▶ 이상(abnormal)을 정의하기 위해 MV(충돌 에너지)의 분포를 확인
# ▶ 대다수의 충돌에너지 data가 충돌에너지의 평균인 60선에 분포
import matplotlib.pyplot as plt 
import seaborn as sns 
get_ipython().run_line_magic("matplotlib", " inline")
# 바로 챠트를 반영해서 보여주기
plt.style.use(['dark_background'])

sns.displot(df_merge['MV'])
plt.gcf().set_size_inches(20,5)
print("총돌에너지 평균 :", df_merge['MV'].mean())


df_merge['MV'].mean() + 2*df_merge['MV'].std()


# ▶ 이상을 정의하는 방법은 다양하고, 보통 현업에 기준이 존재할 확률이 큼
# ▶ 현업의 기준이 존재하지 않는다면, 보통 data의 평균에서 ±1, ±2, ±3 sd(표준편차) 떨어진 Data들을 이상으로 가정하는 경우가 많음
# ▶ mean + 2sd 이상인 충돌 에너지를 이상으로 정의 - 개발자가 정의 한 것임
import numpy as np
df_merge['abnormal'] = np.where( df_merge['MV'] > df_merge['MV'].mean() + 2*df_merge['MV'].std(), 1, 0 ) 
# df_merge[  df_merge['MV'] > 147.19 ] 
df_merge.head()


# ▶ 이상(abnormal) data 분포 확인
df_merge['abnormal'].value_counts()


# ▶ 약 6%가 이상징후를 보이고 있음
# 이상한 것 비율  = (이상한 것 갯수 / 전체수) * 100
# 정상인 것 비율 = ( 정상 갯수 / 전체수) * 100
( 160  / 2800 ) * 100
(2640 / 2800 ) * 100
# 결론 정상 충돌에너지는 94%이고 이상 충돌에너지는 약 6%이다.








# ▶ 이상(abnormal)에 Sensor data 분석
#print("abnormal : ", df_merge[ df_merge['abnormal'] == 1]['s1'].mean(), '/ normal : ', df_merge[ df_merge['abnormal'] == 0]['s1'].mean() )
# ▶ 이상(abnormal)과 정상(normal)일 때, 센서 값의 평균 값이 다른 것을 확인
print("abnormal :",df_merge[df_merge['abnormal']==1]['S1'].mean(), "/ normal : ",df_merge[df_merge['abnormal']==0]['S1'].mean())
print("abnormal :",df_merge[df_merge['abnormal']==1]['S2'].mean(), "/ normal : ",df_merge[df_merge['abnormal']==0]['S2'].mean())
print("abnormal :",df_merge[df_merge['abnormal']==1]['S3'].mean(), "/ normal : ",df_merge[df_merge['abnormal']==0]['S3'].mean())
print("abnormal :",df_merge[df_merge['abnormal']==1]['S4'].mean(), "/ normal : ",df_merge[df_merge['abnormal']==0]['S4'].mean())


# ▶ 더 정확한 분포를 확인하기 위해 Scatter plot(산점도)을 그림
# ▶ 센서값이 중앙(mean)에서 멀어질 수록 이상(abnormal)이 더 많이 발생하고 있는 추세
fig, axes = plt.subplots(4,1)
sns.scatterplot(x=df_merge.indexm y=df_merge['S1'], hue=df_merge['abnormal'], ax=axes[0])


# ▶ 더 정확한 분포를 확인하기 위해 Scatter plot(산점도)을 그림
# ▶ 센서값이 중앙(mean)에서 멀어질 수록 이상(abnormal)이 더 많이 발생하고 있는 추세
fig, axes = plt.subplots(4,1)
sns.scatterplot(x=df_merge.index, y=df_merge['S1'], hue=df_merge['abnormal'], ax=axes[0])
sns.scatterplot(x=df_merge.index, y=df_merge['S2'], hue=df_merge['abnormal'], ax=axes[1])
sns.scatterplot(x=df_merge.index, y=df_merge['S3'], hue=df_merge['abnormal'], ax=axes[2])
sns.scatterplot(x=df_merge.index, y=df_merge['S4'], hue=df_merge['abnormal'], ax=axes[3])
# 각 subplot에 대해 선 그리기
for ax in axes:
    ax.axhline(y=10000, color='r', linewidth=1)
    ax.axhline(y=-10000, color='r', linewidth=1)
plt.gcf().set_size_inches(25,15)





# ▶ S1 불량률 확인
df_s1 = df_merge[ ( df_merge['S1'] > 10000 ) | ( df_merge['S1'] < -10000 ) ]
# 노랑색갯수/278 * 100
df_s1_abnormal_ratio = len( df_s1[ df_s1['abnormal'] == 1 ] ) / len(df_s1) * 100
df_s1_abnormal_ratio


# ▶ S2 불량률 확인
df_s2 = df_merge[ ( df_merge['S2'] > 10000 ) | ( df_merge['S2'] < -10000 ) ]
# 노랑색갯수/278 * 100
df_s2_abnormal_ratio = len( df_s2[ df_s2['abnormal'] == 1 ] ) / len(df_s2) * 100
df_s2_abnormal_ratio


# ▶ S3 불량률 확인
df_s3 = df_merge[ ( df_merge['S3'] > 10000 ) | ( df_merge['S3'] < -10000 ) ]
# 노랑색갯수/278 * 100
df_s3_abnormal_ratio = len( df_s3[ df_s3['abnormal'] == 1 ] ) / len(df_s3) * 100
df_s3_abnormal_ratio


# ▶ S4 불량률 확인
df_s4 = df_merge[ ( df_merge['S4'] > 10000 ) | ( df_merge['S4'] < -10000 ) ]
# 노랑색갯수/278 * 100
df_s4_abnormal_ratio = len( df_s4[ df_s4['abnormal'] == 1 ] ) / len(df_s4) * 100
df_s4_abnormal_ratio


# ▶ 상위의 인사이트를 활용하여 이상을 예측
# ▶ S1, S2, S3, S4가 10,000 초과, -10,000 미만 일시 이상으로 예측



df_merge['rule_base1'] = np.where( ((df_merge['S1'] > 10000) | (df_merge['S1'] < -10000)) , 1, 0)
df_merge['rule_base2'] = np.where( ((df_merge['S2'] > 10000) | (df_merge['S2'] < -10000)) , 1, 0)
df_merge['rule_base3'] = np.where( ((df_merge['S3'] > 10000) | (df_merge['S3'] < -10000)) , 1, 0)
df_merge['rule_base4'] = np.where( ((df_merge['S4'] > 10000) | (df_merge['S4'] < -10000)) , 1, 0)
df_merge.head()


# ▶ presision : 예측한 것 중에 실제로 정답인 비율
# ▶ recall : 실제 정답중에서 예측으로 맞춘 비율
# ▶ S4 센서를 기준으로 이상을 예측 했을 때가 Best Score(recall 0.59)
from sklearn.metrics import classification_report
print(classification_report(df_merge['abnormal'], df_merge['rule_base1']))
print()
print(classification_report(df_merge['abnormal'], df_merge['rule_base2']))
print(classification_report(df_merge['abnormal'], df_merge['rule_base3']))
print(classification_report(df_merge['abnormal'], df_merge['rule_base4']))








from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
X=df_merge.drop(['id', 'M', 'V', 'MV', 'rule_base1', 'rule_base2', 'rule_base3', 'rule_base4', 'abnormal'], axis=1)
Y=df_merge['abnormal']
X # 실제 데이터 값 - 문제
Y # 0 정상 1 이상 - 답
print( X, Y)


from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
X=df_merge.drop(['id', 'M', 'V', 'MV', 'rule_base1', 'rule_base2', 'rule_base3', 'rule_base4', 'abnormal'], axis=1)
Y=df_merge['abnormal']
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)



# ▶ 모델링을 학습하기 위한 Feature(X)와 Y데이터를 구분하는 단계 
print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)





# ▶ 모델 학습
rfc = RandomForestClassifier(random_state=123456)
rfc.fit(x_train, y_train)
# ▶ 예측
y_pred_train = rfc.predict(x_train)
y_pred_test = rfc.predict(x_test)
# ▶ 예측은 학습에 사용된 Data와 Test Data 모두 예측하고 평가함(※ 과적합 여부 판별)
print(classification_report(y_train, y_pred_train))
print(classification_report(y_test, y_pred_test))








# ▶ RandomForestClassifier 객체 생성 후 GridSearchCV 수행



# ▶ Best score 파라미터로 다시 재학습


# ▶ 예측
























# ▶ 학습


# ▶ 예측










